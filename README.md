<div align="center">
  <img src="docs/logo.svg" alt="IP Rule Operator Logo" width="200"/>
  
  # IP Rule Operator

  [![License](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](https://opensource.org/licenses/Apache-2.0)
  [![Go Report Card](https://goreportcard.com/badge/github.com/mariusbertram/ip-rule-operator)](https://goreportcard.com/report/github.com/mariusbertram/ip-rule-operator)
  [![Kubernetes](https://img.shields.io/badge/Kubernetes-1.11%2B-blue.svg)](https://kubernetes.io)
  [![OpenShift](https://img.shields.io/badge/OpenShift-4.x%2B-red.svg)](https://www.openshift.com)

  **Automatische Verwaltung von IP-Routing-Regeln auf Kubernetes-Nodes**
  
  *Policy-Based Routing fÃ¼r Kubernetes LoadBalancer Services*

</div>

---

Ein Kubernetes-Operator zur automatischen Verwaltung von IP-Routing-Regeln auf Cluster-Nodes basierend auf Service LoadBalancer-IPs.

## ğŸ“‹ Ãœberblick

Der **IP Rule Operator** ermÃ¶glicht Policy-Based Routing in Kubernetes-Clustern durch automatische Konfiguration von Linux IP-Regeln auf Cluster-Nodes. Der Operator Ã¼berwacht LoadBalancer-Services und erstellt basierend auf definierten Policies IP-Routing-Regeln, die Traffic von Service ClusterIPs Ã¼ber spezifische Routing-Tabellen leiten.

### Was macht der Operator?

Der Operator besteht aus zwei Hauptkomponenten:

1. **Controller (Manager)**: 
   - Ãœberwacht Kubernetes LoadBalancer-Services
   - Matched LoadBalancer-IPs gegen definierte IPRule-Policies (CIDR-basiert)
   - Generiert automatisch IPRuleConfig-Ressourcen fÃ¼r jeden Service
   - Verwaltet den Agent-DaemonSet

2. **Agent (DaemonSet)**:
   - LÃ¤uft auf jedem Node mit hostNetwork-Zugriff
   - Wendet IP-Routing-Regeln auf dem Node an/entfernt sie
   - Nutzt Linux netlink fÃ¼r direkte Kernel-Interaktion
   - Reconciled kontinuierlich den gewÃ¼nschten Zustand

### Was ist Policy-Based Routing?

**Policy-Based Routing (PBR)** ermÃ¶glicht es, Routing-Entscheidungen nicht nur basierend auf der Ziel-IP-Adresse zu treffen (wie beim klassischen Routing), sondern auch basierend auf anderen Kriterien wie der **Quell-IP-Adresse**.

#### Anwendungsfall im Kubernetes-Kontext:

In einem Kubernetes-Cluster mit mehreren Netzwerk-Interfaces oder Load-Balancern mÃ¶chten Sie mÃ¶glicherweise:

- **Multi-Homing**: Traffic von bestimmten Services Ã¼ber ein spezifisches Netzwerk-Interface leiten
- **Provider-basiertes Routing**: Services verschiedener Mandanten Ã¼ber unterschiedliche ISP-Uplinks routen
- **Traffic-Segregation**: Produktions- und Test-Traffic physisch trennen
- **Geo-Routing**: Traffic basierend auf LoadBalancer-IP-Bereichen regional verteilen

#### Wie funktioniert es?

Der Operator nutzt Linux **IP Rules** (siehe `ip rule`), um Traffic basierend auf der Quell-IP (Service ClusterIP) Ã¼ber alternative Routing-Tabellen zu leiten:

```bash
# Beispiel: Traffic von Service 10.96.1.50 nutzt Routing-Tabelle 100
ip rule add from 10.96.1.50 lookup 100 priority 1000
```

Die Routing-Tabelle 100 kann dann eigene Routes enthalten, z.B.:
```bash
# Tabelle 100: Traffic Ã¼ber spezielles Gateway
ip route add default via 192.168.1.1 dev eth1 table 100
```

**Resultat**: Alle Pakete, die von der Service-IP `10.96.1.50` stammen, werden Ã¼ber das Gateway `192.168.1.1` geroutet, wÃ¤hrend andere Services die Standard-Route nutzen.

## ğŸ—ï¸ Architektur

### Komponenten-Diagramm

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        Kubernetes Cluster                        â”‚
â”‚                                                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚                     Control Plane                           â”‚ â”‚
â”‚  â”‚                                                              â”‚ â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚ â”‚
â”‚  â”‚  â”‚         IP Rule Operator (Controller)                â”‚  â”‚ â”‚
â”‚  â”‚  â”‚                                                       â”‚  â”‚ â”‚
â”‚  â”‚  â”‚  1. Watches Services (LoadBalancer)                  â”‚  â”‚ â”‚
â”‚  â”‚  â”‚  2. Matches LB-IPs against IPRule CRDs              â”‚  â”‚ â”‚
â”‚  â”‚  â”‚  3. Generates IPRuleConfig CRDs                      â”‚  â”‚ â”‚
â”‚  â”‚  â”‚  4. Manages Agent DaemonSet                          â”‚  â”‚ â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚ â”‚
â”‚  â”‚                            â”‚                                 â”‚ â”‚
â”‚  â”‚                            â”‚ watches                         â”‚ â”‚
â”‚  â”‚                            â–¼                                 â”‚ â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚ â”‚
â”‚  â”‚  â”‚              Custom Resources (CRDs)                  â”‚  â”‚ â”‚
â”‚  â”‚  â”‚                                                       â”‚  â”‚ â”‚
â”‚  â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚  â”‚ â”‚
â”‚  â”‚  â”‚  â”‚   IPRule    â”‚  â”‚ IPRuleConfig â”‚  â”‚   Agent    â”‚ â”‚  â”‚ â”‚
â”‚  â”‚  â”‚  â”‚ (User-def.) â”‚  â”‚ (Generated)  â”‚  â”‚ (Deploy)   â”‚ â”‚  â”‚ â”‚
â”‚  â”‚  â”‚  â”‚             â”‚  â”‚              â”‚  â”‚            â”‚ â”‚  â”‚ â”‚
â”‚  â”‚  â”‚  â”‚ cidr: ...   â”‚  â”‚ serviceIP    â”‚  â”‚ image: ... â”‚ â”‚  â”‚ â”‚
â”‚  â”‚  â”‚  â”‚ table: 100  â”‚  â”‚ table: 100   â”‚  â”‚ nodeSelect.â”‚ â”‚  â”‚ â”‚
â”‚  â”‚  â”‚  â”‚ priority    â”‚  â”‚ state: ...   â”‚  â”‚            â”‚ â”‚  â”‚ â”‚
â”‚  â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚  â”‚ â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                                   â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚                        Worker Nodes                         â”‚ â”‚
â”‚  â”‚                                                              â”‚ â”‚
â”‚  â”‚  Node 1                Node 2                Node 3          â”‚ â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚ â”‚
â”‚  â”‚  â”‚  Agent   â”‚         â”‚  Agent   â”‚         â”‚  Agent   â”‚    â”‚ â”‚
â”‚  â”‚  â”‚  Pod     â”‚         â”‚  Pod     â”‚         â”‚  Pod     â”‚    â”‚ â”‚
â”‚  â”‚  â”‚ (DaemonS)â”‚         â”‚ (DaemonS)â”‚         â”‚ (DaemonS)â”‚    â”‚ â”‚
â”‚  â”‚  â”‚          â”‚         â”‚          â”‚         â”‚          â”‚    â”‚ â”‚
â”‚  â”‚  â”‚  Reads   â”‚         â”‚  Reads   â”‚         â”‚  Reads   â”‚    â”‚ â”‚
â”‚  â”‚  â”‚  IPRule  â”‚         â”‚  IPRule  â”‚         â”‚  IPRule  â”‚    â”‚ â”‚
â”‚  â”‚  â”‚  Config  â”‚         â”‚  Config  â”‚         â”‚  Config  â”‚    â”‚ â”‚
â”‚  â”‚  â”‚    â†“     â”‚         â”‚    â†“     â”‚         â”‚    â†“     â”‚    â”‚ â”‚
â”‚  â”‚  â”‚ Applies  â”‚         â”‚ Applies  â”‚         â”‚ Applies  â”‚    â”‚ â”‚
â”‚  â”‚  â”‚ ip rules â”‚         â”‚ ip rules â”‚         â”‚ ip rules â”‚    â”‚ â”‚
â”‚  â”‚  â”‚ via      â”‚         â”‚ via      â”‚         â”‚ via      â”‚    â”‚ â”‚
â”‚  â”‚  â”‚ netlink  â”‚         â”‚ netlink  â”‚         â”‚ netlink  â”‚    â”‚ â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜    â”‚ â”‚
â”‚  â”‚       â”‚                    â”‚                    â”‚           â”‚ â”‚
â”‚  â”‚       â–¼                    â–¼                    â–¼           â”‚ â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚ â”‚
â”‚  â”‚  â”‚          Linux Kernel Routing Tables                â”‚   â”‚ â”‚
â”‚  â”‚  â”‚                                                      â”‚   â”‚ â”‚
â”‚  â”‚  â”‚  ip rule show:                                      â”‚   â”‚ â”‚
â”‚  â”‚  â”‚    1000: from 10.96.1.50 lookup 100                â”‚   â”‚ â”‚
â”‚  â”‚  â”‚    1000: from 10.96.1.51 lookup 200                â”‚   â”‚ â”‚
â”‚  â”‚  â”‚    ...                                               â”‚   â”‚ â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Ressourcen-Interaktion

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Workflow & Interaktionen                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

  User/Admin                     Kubernetes API
      â”‚                                â”‚
      â”‚ 1. Create IPRule               â”‚
      â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€>â”‚
      â”‚    (cidr: 192.168.1.0/24)      â”‚
      â”‚    (table: 100, priority: 1000)â”‚
      â”‚                                â”‚
      â”‚ 2. Create Service (LB)         â”‚
      â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€>â”‚
      â”‚    (LoadBalancer)              â”‚
      â”‚                                â”‚
                                       â”‚
                      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                      â”‚                                  â”‚
                      â–¼                                  â”‚
           IP Rule Controller                            â”‚
                      â”‚                                  â”‚
        3. Watches Services & IPRules                    â”‚
                      â”‚                                  â”‚
        4. Service gets LB IP: 192.168.1.10              â”‚
           ClusterIP: 10.96.1.50                         â”‚
                      â”‚                                  â”‚
        5. Matches: 192.168.1.10 âˆˆ 192.168.1.0/24       â”‚
                      â”‚                                  â”‚
        6. Creates IPRuleConfig â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€>â”‚
           - name: iprc-10-96-1-50                       â”‚
           - serviceIP: 10.96.1.50                       â”‚
           - table: 100                                  â”‚
           - priority: 1000                              â”‚
           - state: present                              â”‚
                                                         â”‚
                      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                      â”‚
                      â–¼
              Agent Controller
                      â”‚
        7. Ensures Agent DaemonSet exists
                      â”‚
                      â”œâ”€â”€â”€> Creates/Updates DaemonSet
                      â”‚
                      â–¼
            Agent Pods (on each node)
                      â”‚
        8. Reconcile Loop (every 10s):
           - List all IPRuleConfig
           - Read current ip rules (netlink)
                      â”‚
        9. For state=present:
           â”œâ”€> Check if rule exists
           â””â”€> If not: ip rule add from 10.96.1.50 
                          lookup 100 priority 1000
                      â”‚
       10. For state=absent:
           â”œâ”€> Check if rule exists
           â””â”€> If yes: ip rule del from 10.96.1.50
                          lookup 100 priority 1000
                      â”‚
                      â–¼
           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
           â”‚  Linux Kernel        â”‚
           â”‚  Routing Applied     â”‚
           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  Cleanup Workflow (Service deleted)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    Service Deleted
          â”‚
          â–¼
    IP Rule Controller
          â”‚
    11. LoadBalancer IP no longer exists
          â”‚
    12. Updates IPRuleConfig:
        - state: present â†’ absent
          â”‚
          â–¼
    Agent Pods
          â”‚
    13. Detects state=absent
          â”‚
    14. Removes ip rule from node
        (ip rule del from 10.96.1.50 ...)
          â”‚
          â–¼
    Cleanup Complete
```

## ğŸš€ Installation

### Voraussetzungen

- Kubernetes v1.11.3+ oder OpenShift 4.x+
- kubectl oder oc CLI konfiguriert
- Cluster-Admin-Rechte fÃ¼r Installation
- Linux-Nodes (Agent benÃ¶tigt Linux-Kernel mit netlink-Support)

### Methode 1: Installation via YAML (Kubernetes)

#### Schritt 1: CRDs installieren

```bash
kubectl apply -f https://raw.githubusercontent.com/mariusbertram/ip-rule-operator/main/config/crd/bases/api.operator.brtrm.dev_iprules.yaml
kubectl apply -f https://raw.githubusercontent.com/mariusbertram/ip-rule-operator/main/config/crd/bases/api.operator.brtrm.dev_ipruleconfigs.yaml
kubectl apply -f https://raw.githubusercontent.com/mariusbertram/ip-rule-operator/main/config/crd/bases/api.operator.brtrm.dev_agents.yaml
```

Oder mit Makefile (aus Repository):
```bash
make install
```

#### Schritt 2: Operator deployen

Mit vorgebauten Images:
```bash
kubectl apply -f https://raw.githubusercontent.com/mariusbertram/ip-rule-operator/main/dist/install.yaml
```

Oder selbst bauen und deployen:
```bash
export IMG=ghcr.io/mariusbertram/iprule-controller:v0.0.1
export AGENT_IMG=ghcr.io/mariusbertram/iprule-agent:v0.0.1

# Images bauen und pushen
make docker-build-all docker-push-all

# Operator deployen
make deploy IMG=${IMG}
```

#### Schritt 3: Agent-DaemonSet erstellen

```bash
cat <<EOF | kubectl apply -f -
apiVersion: api.operator.brtrm.dev/v1alpha1
kind: Agent
metadata:
  name: iprule-agent
  namespace: ip-rule-operator-system
spec:
  # Optional: Spezifisches Image
  # image: ghcr.io/mariusbertram/iprule-agent:v0.0.1
  
  # Optional: Node-Selektor
  nodeSelector:
    kubernetes.io/os: linux
  
  # Optional: Tolerations fÃ¼r Control-Plane-Nodes
  tolerations:
  - key: node-role.kubernetes.io/control-plane
    operator: Exists
    effect: NoSchedule
EOF
```

#### Schritt 4: Verifizierung

```bash
# PrÃ¼fe Operator-Pod
kubectl get pods -n ip-rule-operator-system

# PrÃ¼fe Agent-DaemonSet
kubectl get daemonset -n ip-rule-operator-system iprule-agent

# PrÃ¼fe CRDs
kubectl get crds | grep api.operator.brtrm.dev
```

### Methode 2: Installation via OLM (OpenShift)

Der Operator kann Ã¼ber den Operator Lifecycle Manager (OLM) in OpenShift installiert werden.

#### Option A: Ãœber OpenShift Web Console

1. **Ã–ffne die OpenShift Web Console**
2. Navigiere zu **Operators** â†’ **OperatorHub**
3. Suche nach **"IP Rule Operator"**
4. Klicke auf **Install**
5. WÃ¤hle:
   - **Update Channel**: stable
   - **Installation Mode**: All namespaces on the cluster
   - **Installed Namespace**: openshift-operators
   - **Update Approval**: Automatic (empfohlen)
6. Klicke auf **Install**
7. Warte, bis der Status **Succeeded** anzeigt

#### Option B: Via CLI (oc)

##### Schritt 1: CatalogSource erstellen

```bash
cat <<EOF | oc apply -f -
apiVersion: operators.coreos.com/v1alpha1
kind: CatalogSource
metadata:
  name: ip-rule-operator-catalog
  namespace: openshift-marketplace
spec:
  sourceType: grpc
  image: ghcr.io/mariusbertram/ip-rule-operator-catalog:v0.0.1
  displayName: IP Rule Operator Catalog
  publisher: Marius Bertram
  updateStrategy:
    registryPoll:
      interval: 10m
EOF
```

##### Schritt 2: OperatorGroup erstellen (falls nicht vorhanden)

```bash
cat <<EOF | oc apply -f -
apiVersion: operators.coreos.com/v1
kind: OperatorGroup
metadata:
  name: global-operators
  namespace: openshift-operators
spec: {}
EOF
```

##### Schritt 3: Subscription erstellen

```bash
cat <<EOF | oc apply -f -
apiVersion: operators.coreos.com/v1alpha1
kind: Subscription
metadata:
  name: ip-rule-operator
  namespace: openshift-operators
spec:
  channel: stable
  name: ip-rule-operator
  source: ip-rule-operator-catalog
  sourceNamespace: openshift-marketplace
  installPlanApproval: Automatic
EOF
```

##### Schritt 4: Installation verifizieren

```bash
# PrÃ¼fe Subscription-Status
oc get subscription ip-rule-operator -n openshift-operators

# PrÃ¼fe InstallPlan
oc get installplan -n openshift-operators

# PrÃ¼fe ClusterServiceVersion (CSV)
oc get csv -n openshift-operators | grep ip-rule

# PrÃ¼fe Operator-Pod
oc get pods -n openshift-operators | grep ip-rule

# PrÃ¼fe CRDs
oc get crds | grep api.operator.brtrm.dev
```

##### Schritt 5: Agent-Instanz erstellen

Nach erfolgreicher Installation des Operators:

```bash
cat <<EOF | oc apply -f -
apiVersion: api.operator.brtrm.dev/v1alpha1
kind: Agent
metadata:
  name: iprule-agent
  namespace: openshift-operators
spec:
  nodeSelector:
    kubernetes.io/os: linux
  tolerations:
  - key: node-role.kubernetes.io/master
    operator: Exists
    effect: NoSchedule
  - key: node-role.kubernetes.io/control-plane
    operator: Exists
    effect: NoSchedule
EOF
```

#### OLM Bundle lokal bauen und pushen

FÃ¼r Entwickler/Maintainer:

```bash
# Bundle generieren
make bundle VERSION=0.0.1

# Bundle-Image bauen
make bundle-build BUNDLE_IMG=ghcr.io/mariusbertram/ip-rule-operator-bundle:v0.0.1

# Bundle-Image pushen
make bundle-push BUNDLE_IMG=ghcr.io/mariusbertram/ip-rule-operator-bundle:v0.0.1

# Catalog-Image bauen (File-Based Catalog)
make catalog-fbc-build CATALOG_IMG=ghcr.io/mariusbertram/ip-rule-operator-catalog:v0.0.1

# Catalog-Image pushen
make catalog-push CATALOG_IMG=ghcr.io/mariusbertram/ip-rule-operator-catalog:v0.0.1
```

### Deinstallation

#### Kubernetes (YAML):

```bash
# Operator entfernen
make undeploy

# CRDs entfernen
make uninstall
```

#### OpenShift (OLM):

```bash
# Via CLI
oc delete subscription ip-rule-operator -n openshift-operators
oc delete csv -n openshift-operators $(oc get csv -n openshift-operators | grep ip-rule | awk '{print $1}')
oc delete catalogsource ip-rule-operator-catalog -n openshift-marketplace

# Oder via Web Console:
# Operators â†’ Installed Operators â†’ IP Rule Operator â†’ Uninstall
```

## ğŸ“– Verwendung

### Beispiel 1: Einfache IP-Regel

Erstelle eine IPRule fÃ¼r LoadBalancer-IPs im Bereich `192.168.1.0/24`:

```yaml
apiVersion: api.operator.brtrm.dev/v1alpha1
kind: IPRule
metadata:
  name: datacenter-a-routing
spec:
  cidr: "192.168.1.0/24"
  table: 100
  priority: 1000
```

```bash
kubectl apply -f iprule-example.yaml
```

**Effekt**: Alle Services mit LoadBalancer-IPs in diesem Bereich werden automatisch mit IP-Regeln konfiguriert, die Routing-Tabelle 100 verwenden.

### Beispiel 2: Multi-Datacenter-Setup

```yaml
---
apiVersion: api.operator.brtrm.dev/v1alpha1
kind: IPRule
metadata:
  name: datacenter-a
spec:
  cidr: "10.0.0.0/16"
  table: 100
  priority: 1000
---
apiVersion: api.operator.brtrm.dev/v1alpha1
kind: IPRule
metadata:
  name: datacenter-b
spec:
  cidr: "10.1.0.0/16"
  table: 200
  priority: 1000
---
apiVersion: api.operator.brtrm.dev/v1alpha1
kind: IPRule
metadata:
  name: datacenter-c-priority
spec:
  cidr: "10.2.0.0/16"
  table: 300
  priority: 2000
```

**Anwendungsfall**: Services mit LB-IPs aus unterschiedlichen Datacenter-Bereichen nutzen verschiedene Routing-Tabellen (z.B. fÃ¼r verschiedene ISP-Uplinks).

### Beispiel 3: Routing-Tabellen konfigurieren

Die IP-Regeln verweisen auf Routing-Tabellen. Diese mÃ¼ssen auf den Nodes konfiguriert werden:

```bash
# Auf jedem Node:
# /etc/iproute2/rt_tables erweitern
echo "100 datacenter_a" >> /etc/iproute2/rt_tables
echo "200 datacenter_b" >> /etc/iproute2/rt_tables

# Routes in Tabelle 100 konfigurieren
ip route add default via 192.168.1.1 dev eth1 table 100

# Routes in Tabelle 200 konfigurieren
ip route add default via 192.168.2.1 dev eth2 table 200

# Persistenz mit NetworkManager oder systemd-networkd sicherstellen
```

### Status prÃ¼fen

```bash
# IPRules anzeigen
kubectl get iprules

# IPRuleConfigs anzeigen (automatisch generiert)
kubectl get ipruleconfigs

# Agent-Status prÃ¼fen
kubectl get agent -n ip-rule-operator-system

# Agent-Logs anzeigen
kubectl logs -n ip-rule-operator-system -l app=iprule-agent --tail=100

# Controller-Logs
kubectl logs -n ip-rule-operator-system deployment/ip-rule-operator-controller-manager --tail=100

# IP-Regeln auf einem Node prÃ¼fen
kubectl debug node/<node-name> -it --image=nicolaka/netshoot
ip rule show
```

## ğŸ”§ Entwicklung

### Lokale Entwicklung

#### Voraussetzungen
- Go 1.24+
- Docker oder Podman
- kubectl
- operator-sdk v1.41.1+
- Access zu einem Kubernetes-Cluster (z.B. Kind, Minikube)

#### Setup

```bash
# Repository klonen
git clone https://github.com/mariusbertram/ip-rule-operator.git
cd ip-rule-operator

# Dependencies installieren
go mod download

# Code generieren
make generate manifests

# Tests ausfÃ¼hren
make test

# Lokaler Build
make build build-agent
```

#### Lokales Deployment

```bash
# Kind-Cluster erstellen
kind create cluster --name ip-rule-operator-dev

# CRDs installieren
make install

# Controller lokal ausfÃ¼hren (auÃŸerhalb des Clusters)
make run

# In einem anderen Terminal: Agent lokal ausfÃ¼hren (nur Linux)
# ACHTUNG: BenÃ¶tigt NET_ADMIN Capability
sudo make run-agent
```

#### Images bauen und pushen

```bash
# Setze Registry
export REGISTRY=ghcr.io/yourusername/

# Beide Images bauen
make docker-build-all VERSION=0.0.2

# Beide Images pushen
make docker-push-all VERSION=0.0.2

# In Cluster deployen
make deploy IMG=ghcr.io/yourusername/iprule-controller:v0.0.2
```

### E2E-Tests

```bash
# E2E-Tests mit Kind
make test-e2e

# Manuelles Setup fÃ¼r E2E
make setup-test-e2e
# Tests ausfÃ¼hren
go test ./test/e2e/ -v -ginkgo.v
# Cleanup
make cleanup-test-e2e
```

### Code-QualitÃ¤t

```bash
# Linting
make lint

# Linting mit Auto-Fix
make lint-fix

# Formatting
make fmt

# Vet
make vet
```

## ğŸ¤ Contributing

BeitrÃ¤ge sind willkommen! Bitte beachte:

1. **Fork** das Repository
2. Erstelle einen **Feature-Branch** (`git checkout -b feature/amazing-feature`)
3. **Commit** deine Ã„nderungen (`git commit -m 'Add amazing feature'`)
4. **Push** zum Branch (`git push origin feature/amazing-feature`)
5. Ã–ffne einen **Pull Request**

### Code-Richtlinien

- Folge den Go-Coding-Standards
- FÃ¼ge Tests fÃ¼r neue Features hinzu
- Aktualisiere die Dokumentation
- FÃ¼hre `make lint` und `make test` vor dem Commit aus

## ğŸ“ Lizenz

Copyright 2025 Marius Bertram.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.

## ğŸ”— Links

- [Operator SDK Documentation](https://sdk.operatorframework.io/)
- [Kubebuilder Documentation](https://book.kubebuilder.io/)
- [Linux Policy Routing](https://www.kernel.org/doc/html/latest/networking/policy-routing.html)
- [iproute2 Documentation](https://wiki.linuxfoundation.org/networking/iproute2)

## ğŸ“ Support

Bei Fragen oder Problemen:
- Ã–ffne ein [Issue](https://github.com/mariusbertram/ip-rule-operator/issues)
- Kontaktiere: Marius Bertram

---

**â­ Wenn dir dieses Projekt gefÃ¤llt, gib ihm einen Stern auf GitHub!**

